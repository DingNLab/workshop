{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Psycholinguisitcs\n",
    "主要内容：使用自然语言处理模型提取语言特征和进行语言任务"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 加载工具包\n",
    "\n",
    "* 使用到的自然语言处理工具包\n",
    "    - **srilm**: 计算频次、转移概率。[[官方文档]](https://srilm-python.readthedocs.io/en/latest/#)。安装srilm可参考[[安装教程]](https://github.com/zhaoyanpeng/pysrilm)\n",
    "    - **hanlp**: 适用于基本自然语言处理任务，包括分词、词性分析、句法分析、语义分析、静态词向量提取等等，对于中文比较友好。[[官方文档]](https://hanlp.hankcs.com/docs/)\n",
    "    - **Huggingface系列**: 调用开源深度学习模型完成以上两者提到的，以及更多其他的任务，包括情感分析、文本生成等等。[[官网]](https://huggingface.co/)\n",
    "* 其他常用的自然语言处理工具包\n",
    "    - **nltk**：可以调用众多语料库（如wordnet等），也可以进行一系列的自然语言处理任务。[[官方文档]](https://www.nltk.org/)\n",
    "    - **spacy**：速度快、功能全面的自然语言处理工具包。[[官方文档]](https://spacy.io/)\n",
    "    - **stanza**：Stanford CoreNLP的python版本\n",
    "    - **fastNLP**：复旦大学制作的NLP工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install srilm: Refer to https://github.com/zhaoyanpeng/pysrilm\n",
    "\n",
    "# 如果在colab等服务器上运行，先用以下命令去掉#安装工具包\n",
    "#!pip install hanlp\n",
    "#!pip install transformers, tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果只需要提取一部分特征，可以选择性地导入以下工具包\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据处理及可视化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc(\"font\", family='SimHei') # 用来显示中文，对于macos系统需要换一个支持的字体\n",
    "\n",
    "# 自然语言处理\n",
    "from srilm import LM\n",
    "import hanlp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    GPT2LMHeadModel, \n",
    "    TextGenerationPipeline,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 词汇及语义特征提取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 数据预处理：加载语料库以及进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_str(astr, tokenizer):\n",
    "    '''\n",
    "    # 使用分词模型来分词\n",
    "    输入: \n",
    "        astr: str, a sentence\n",
    "        tokenizer: hanlp tokenizer\n",
    "    输出:\n",
    "        a sentence with words separated by space\n",
    "    '''\n",
    "    words = tokenizer(astr)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def prepare_corpus(tokenizer, corpus, save_json_name):\n",
    "    '''\n",
    "    # 对语料库进行分词\n",
    "    输入:\n",
    "        tokenizer: hanlp tokenizer\n",
    "        corpus: str, the path of corpus\n",
    "        save_json_name: str, the path of saving json file\n",
    "    输出: \n",
    "        \n",
    "    '''\n",
    "    with open(save_json_name, 'r', encoding='utf-8') as fp:\n",
    "        wiki_texts = json.load(fp)\n",
    "        wiki_texts_new = []\n",
    "        for line in tqdm(wiki_texts):\n",
    "            wiki_texts_new.append(filter_str(line, tokenizer))\n",
    "        open(corpus, 'w').write('\\n'.join(wiki_texts_new))\n",
    "\n",
    "# 加载hanlp中的分词模型\n",
    "hanlp_tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "# wiki语料\n",
    "wiki_file = './srilm_data_model/wiki_demo/wiki_z.json'\n",
    "# 分词后语料文件\n",
    "wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'\n",
    "# 执行\n",
    "prepare_corpus(hanlp_tok, wiki_file_tkd, wiki_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基于语料库统计的N-gram计算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 从语料库中生成N-gram模型\n",
    "* 将语料库（corpus）和指定的模型设置（ngram）输入模型，在模型存储路径（model_path）中输出统计好的模型\n",
    "* 现成的N-gram语料库：[google n-gram](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: discount coeff 1 is out of range: 0\n",
      "warning: discount coeff 7 is out of range: 1.91919\n"
     ]
    }
   ],
   "source": [
    "def generate_model(model_path, ngram, corpus):\n",
    "    '''\n",
    "    输入:\n",
    "        model_path: str, ngram模型的保存路径\n",
    "        ngram: str, ngram-count路径\n",
    "        corpus: str, corpus路径\n",
    "    输出:\n",
    "        \n",
    "    '''\n",
    "    cmd = '{} -text {} -order 3 -kndiscount3 -lm {}'.format(ngram, corpus, model_path)\n",
    "    os.system(cmd)\n",
    "\n",
    "ngram = '/home/zhang/acoustic_theory/workspace/21-12-30-srilm/srilm/bin/i686-m64/ngram-count'\n",
    "wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'\n",
    "model_path = './srilm_data_model/wiki_demo/wiki_z_word.lm'\n",
    "generate_model(model_path, ngram, wiki_file_tkd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './srilm_data_model/wiki/wiki_z_word.lm'\n",
    "lm = LM(model_path, lower=True) # 加载N-gram模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 采用N-gram模型计算词频\n",
    "用srilm的LM来调用刚刚生成的模型，采用`lm.logprob_strings(word, context)`来生成 $\\log{p \\left( \\rm{word} | context \\right)}$，word是当前单词，当context是空列表`[]`时相当于1-gram即词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 计算词频 ********************\n",
      "====================P(的) vs P(西瓜) vs P(桌子)====================\n",
      "P(的): -1.3277089595794678\n",
      "P(西瓜): -5.5793938636779785\n",
      "P(桌子): -5.5162577629089355\n"
     ]
    }
   ],
   "source": [
    "# 计算词频\n",
    "print('*'*20 + ' 计算词频 ' + '*'*20)\n",
    "word_freq0_ = lm.logprob_strings('的', [])\n",
    "word_freq1_ = lm.logprob_strings('西瓜', [])\n",
    "word_freq2_ = lm.logprob_strings('桌子', [])\n",
    "\n",
    "# 输出结果\n",
    "print('='*20 + 'P(的) vs P(西瓜) vs P(桌子)' + '='*20)\n",
    "print('P(的): ' + str(word_freq0_))\n",
    "print('P(西瓜): ' + str(word_freq1_))\n",
    "print('P(桌子): ' + str(word_freq2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 采用N-gram模型计算转移概率\n",
    "\n",
    "当$n>1$时，在`context`中放入前$n-1$个词，顺序是从右到左。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========P(西瓜 | 吃, 喜欢) vs P(桌子 | 吃, 喜欢)==========\n",
      "P(西瓜 | 吃, 喜欢): -2.884925365447998\n",
      "P(桌子 | 吃, 喜欢): -6.211382865905762\n"
     ]
    }
   ],
   "source": [
    "tp1_ = lm.logprob_strings('西瓜', ['吃', '喜欢'])\n",
    "tp2_ = lm.logprob_strings('桌子', ['吃', '喜欢'])\n",
    "print('='*10 + 'P(西瓜 | 吃, 喜欢) vs P(桌子 | 吃, 喜欢)' + '='*10)\n",
    "print('P(西瓜 | 吃, 喜欢): ' + str(tp1_))\n",
    "print('P(桌子 | 吃, 喜欢): ' + str(tp2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 采用N-gram模型计算surprisal\n",
    "$\\rm{surprisal} = -\\log{ \\it{p} \\left( \\rm{word} | context \\right)}$，所以只要取负即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========surprisal(西瓜 | 吃, 喜欢) vs surprisal(桌子 | 吃, 喜欢)==========\n",
      "surprisal(西瓜 | 吃, 喜欢): 2.884925365447998\n",
      "surprisal(桌子 | 吃, 喜欢): 6.211382865905762\n"
     ]
    }
   ],
   "source": [
    "s1_ = -lm.logprob_strings('西瓜', ['吃', '喜欢'])\n",
    "s2_ = -lm.logprob_strings('桌子', ['吃', '喜欢'])\n",
    "print('='*10 + 'surprisal(西瓜 | 吃, 喜欢) vs surprisal(桌子 | 吃, 喜欢)' + '='*10)\n",
    "print('surprisal(西瓜 | 吃, 喜欢): ' + str(s1_))\n",
    "print('surprisal(桌子 | 吃, 喜欢): ' + str(s2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 采用N-gram模型计算entropy\n",
    "$\\rm{entropy} = \\sum \\left( p*surprisal \\right)$，所以对于给定的context，对所有的词来计算surprisal然后求期望"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========entropy(蝴) vs entropy(。)==========\n",
      "entropy(蝴): 0.03182660213747036\n",
      "entropy(。): 2.5136258206385347\n"
     ]
    }
   ],
   "source": [
    "model_path = './srilm_data_model/wiki/wiki_z_morpheme.lm'\n",
    "lm = LM(model_path, lower=True) # 加载N-gram模型\n",
    "def entropy_cal(lm, context):\n",
    "    # entropy\n",
    "    raw_text_idx = [lm.vocab.intern(w) for w in context]\n",
    "    vocab_num = lm.vocab.max_interned() + 1\n",
    "    logprobs = [lm.logprob(i, raw_text_idx) for i in range(vocab_num)]\n",
    "    logprobs_np = np.array(logprobs)\n",
    "    logprobs_np_ = logprobs_np[logprobs_np > -np.inf]\n",
    "    entropy_ = sum(-np.power(10, logprobs_np_)*logprobs_np_)\n",
    "    return entropy_\n",
    "\n",
    "print('='*10 + 'entropy(蝴) vs entropy(。)' + '='*10)\n",
    "e1_ = entropy_cal(lm, ['蝴'])\n",
    "print('entropy(蝴): ' + str(e1_))\n",
    "e2_ = entropy_cal(lm, ['。'])\n",
    "print('entropy(。): ' + str(e2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 基于深度学习模型的转移概率计算\n",
    "\n",
    "以gpt-2为例，采用的模型为[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 加载模型，包括分词模型与语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "ckpt_path = \"uer/gpt2-chinese-cluecorpussmall\" # checkpoint模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(ckpt_path) # 分词器\n",
    "model = GPT2LMHeadModel.from_pretrained(ckpt_path) # 语言模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 获取模型的转移概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========输入tokens: ==========\n",
      "['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵', '羊', '吃', '草', '。', '[SEP]']\n",
      "==========转移概率维度: ==========\n",
      "torch.Size([12, 21128])  输入字数 x 总字数\n"
     ]
    }
   ],
   "source": [
    "model.config.output_hidden_states = True  # 在模型设置config中设置为True，可以让模型输出hidden states\n",
    "inputs = tokenizer('蝴蝶飞舞。绵羊吃草。', return_tensors=\"pt\") # 对句子进行分词\n",
    "\n",
    "tks = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print('='*10 + '输入tokens: ' + '='*10)\n",
    "print(tks)\n",
    "\n",
    "outputs = model(**inputs)  # 将分词后的句子输入模型，得到模型输出的结果\n",
    "probs = outputs.logits[0]\n",
    "print('='*10 + '转移概率维度: ' + '='*10)\n",
    "print(str(probs.shape) + '  输入字数 x 总字数')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 获取模型的surprisal与entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]']\n",
      "tokens (top k): ['如', '很', '这', '有', '不', '书', '你', '为', '我', '一']\n",
      "surprisal of 蝴: 4.849288742842096; entropy: 2.7137670516967773; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴']\n",
      "tokens (top k): ['蝶', '蜓', '[UNK]', '蛹', '-', '蝴', '蜢', '##ser', '##e', '~']\n",
      "surprisal of 蝶: 0.00013581902527431; entropy: 0.002098201308399439; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶']\n",
      "tokens (top k): ['蝶', '（', '变', '的', '是', '酥', '属', '超', '结', '飞']\n",
      "surprisal of 飞: 1.875799630285334; entropy: 2.705556869506836; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞']\n",
      "tokens (top k): ['机', '行', '蝶', '蛾', '舞', '翔', '碟', '鸟', '鱼', '龙']\n",
      "surprisal of 舞: 1.3007874730402909; entropy: 1.6138794422149658; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞']\n",
      "tokens (top k): ['蝴', '蝶', '的', '是', '（', '(', '，', '-', '[SEP]', '《']\n",
      "surprisal of 。: 2.868855494287529; entropy: 2.5130739212036133; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。']\n",
      "tokens (top k): ['蝴', '[SEP]', '蝶', '这', '一', '是', '。', '我', '不', '飞']\n",
      "surprisal of 绵: 4.865470369658404; entropy: 2.337616443634033; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵']\n",
      "tokens (top k): ['绵', '羊', '长', '密', '綿', '阳', '[UNK]', '延', '软', '柔']\n",
      "surprisal of 羊: 1.1589601338270719; entropy: 0.8466951251029968; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵', '羊']\n",
      "tokens (top k): ['羔', '毛', '脂', '蝎', '角', '，', '驼', '绒', '羊', '年']\n",
      "surprisal of 吃: 4.274023174230079; entropy: 2.755201816558838; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵', '羊', '吃']\n",
      "tokens (top k): ['了', '羊', '的', '。', '草', '食', '起', '完', '，', '饭']\n",
      "surprisal of 草: 1.705819622305031; entropy: 2.481321334838867; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵', '羊', '吃', '草']\n",
      "tokens (top k): ['。', '莓', '，', '原', '的', '蜢', '（', '地', '草', '[SEP]']\n",
      "surprisal of 。: 0.7054180647842639; entropy: 1.960336446762085; \n",
      "\n",
      "==========================================\n",
      "previous tokens: ['[CLS]', '蝴', '蝶', '飞', '舞', '。', '绵', '羊', '吃', '草', '。']\n",
      "tokens (top k): ['[SEP]', '绵', '这', '我', '羊', '。', '蝴', '一', '小', '不']\n",
      "surprisal of [SEP]: 0.6934596181641993; entropy: 2.7353546619415283; \n"
     ]
    }
   ],
   "source": [
    "probs_sfm = F.softmax(probs, dim=-1)\n",
    "input_ids_ = inputs['input_ids'][0]\n",
    "prob_target = [probs_sfm[idx, in_id_tmp].item() for idx, in_id_tmp in enumerate(input_ids_[1:])]\n",
    "\n",
    "gpt_surprisal = -np.log10(prob_target)\n",
    "gpt_entropy = -(torch.log10(probs_sfm) * probs_sfm).nansum(dim=-1)\n",
    "tks = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for idx in range(len(tks)-1):\n",
    "    print('\\n==========================================')\n",
    "    print(f'previous tokens: {tks[:idx+1]}')\n",
    "\n",
    "    prob_sort_idx = probs_sfm[idx, :].argsort(descending=True)\n",
    "    pred_tks = tokenizer.convert_ids_to_tokens(prob_sort_idx)\n",
    "    print(f'tokens (top k): {pred_tks[:10]}')\n",
    "    # print(f'prob of tokens (top k): {probs_sfm[idx, prob_sort_idx[:10]]}')\n",
    "\n",
    "    print(f'surprisal of {tks[idx+1]}: {gpt_surprisal[idx]}; entropy: {gpt_entropy[idx]}; ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building model \u001b[5m\u001b[33m...\u001b[0m\u001b[0m          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 分词结果：\n",
      "['这个', '门', '被', '锁', '了', '，', '锁', '很难', '被', '打开', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 词性标注：\n",
      "['DT', 'NN', 'SB', 'VV', 'SP', 'PU', 'VV', 'AD', 'SB', 'VV', 'PU']\n"
     ]
    }
   ],
   "source": [
    "## 0. 分词\n",
    "sent_ex = '这个门被锁了，锁很难被打开。'\n",
    "tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "tks = tok(sent_ex)\n",
    "print('0. 分词结果：')\n",
    "print(tks)\n",
    "\n",
    "## 1. 词性标注\n",
    "pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)\n",
    "print('1. 词性标注：')\n",
    "print(pos(tks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 词向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 获取静态词向量：以word2vec为例\n",
    "* hanlp支持调用各种静态词向量， 包括[word2vec](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/word2vec.html), [glove](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/glove.html)等等，具体的模型及文献可以在链接文档中进行选择，一般情况下维度越高越准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4234e-02,  8.3600e-02,  2.4145e-02, -1.0256e-01, -1.0829e-01,\n",
       "        -2.6786e-02, -9.6481e-02,  9.0537e-02, -5.4941e-02,  4.5936e-02,\n",
       "        -4.2577e-02, -5.1776e-02,  4.9661e-02, -3.2703e-02, -6.6407e-03,\n",
       "         9.8313e-03,  4.2377e-02, -7.1969e-02,  6.7363e-02, -1.2679e-01,\n",
       "         1.3423e-03,  1.8129e-02,  1.3923e-02,  6.0298e-02,  2.9974e-02,\n",
       "         3.4969e-02,  4.7053e-02, -1.4874e-02,  6.6235e-02, -1.5579e-01,\n",
       "        -1.1716e-01,  8.8726e-02,  6.0976e-02, -8.0692e-02, -3.1017e-02,\n",
       "        -1.3132e-02,  5.4841e-02,  4.0733e-02, -1.5295e-01, -7.8516e-02,\n",
       "         6.6119e-02,  2.9393e-02, -3.0162e-02, -4.3704e-02,  8.3047e-03,\n",
       "        -7.7654e-02, -1.5644e-02,  6.2678e-02,  7.3149e-02, -1.9128e-02,\n",
       "         2.7543e-02, -1.4893e-02, -1.2223e-02,  9.6474e-02,  2.1985e-02,\n",
       "         4.4640e-02, -2.4626e-02,  9.8536e-02, -1.3777e-01,  5.1621e-02,\n",
       "         9.5042e-02, -3.2784e-02,  2.8697e-02, -1.3267e-02,  1.1536e-02,\n",
       "        -9.0047e-02, -7.2654e-02, -8.7082e-04, -3.6991e-02,  1.6448e-03,\n",
       "         2.6809e-02, -7.5198e-02, -2.6094e-02,  6.5516e-03, -7.2922e-02,\n",
       "        -6.3720e-02, -6.4798e-03,  1.3006e-02,  1.7040e-02, -4.3527e-02,\n",
       "         1.6448e-03, -4.0217e-02,  2.1293e-02, -4.1442e-02, -4.9964e-02,\n",
       "         1.0784e-02,  1.2986e-01, -1.7174e-02,  9.0332e-02,  8.1890e-04,\n",
       "        -4.3150e-02, -6.7029e-02, -4.6127e-02, -6.4486e-02, -1.8022e-02,\n",
       "         1.3425e-02,  6.9962e-02, -1.4400e-02,  6.0225e-03, -3.7480e-03,\n",
       "         8.5195e-03, -2.2870e-02, -4.1049e-02, -1.8603e-02, -5.3075e-02,\n",
       "        -7.1510e-02,  9.2589e-03, -6.3029e-03, -2.4524e-02, -3.4340e-02,\n",
       "        -8.8730e-02,  1.5332e-02,  2.8820e-02,  1.8295e-02, -5.8320e-02,\n",
       "        -2.7167e-02, -1.7402e-02, -7.7428e-02, -1.0769e-01, -1.0446e-01,\n",
       "         4.5363e-02, -6.3230e-02,  8.3784e-02,  5.3965e-02,  2.0121e-02,\n",
       "        -3.7716e-02, -2.0752e-02, -6.2321e-02, -1.3778e-01,  5.0385e-02,\n",
       "         8.9087e-06, -8.1429e-02,  6.1611e-02, -4.1132e-02,  7.4521e-02,\n",
       "        -5.0390e-02, -1.6549e-02,  4.1053e-02, -1.7056e-02, -1.2268e-02,\n",
       "        -1.3683e-02,  1.0725e-02, -5.9534e-02, -3.3246e-02,  3.8279e-02,\n",
       "        -3.6564e-02,  6.8516e-02,  6.6845e-02,  4.3522e-02, -2.3375e-02,\n",
       "        -1.3111e-02,  1.4433e-03,  3.9912e-02,  3.8543e-03,  8.9713e-02,\n",
       "         1.9988e-02,  9.5058e-04, -7.2403e-02, -3.7107e-02, -6.4932e-02,\n",
       "        -2.1959e-02,  3.4034e-02, -2.9596e-02, -6.8593e-02, -1.9584e-02,\n",
       "         4.0717e-02, -1.0285e-01, -6.5889e-03,  9.2453e-03, -4.2289e-02,\n",
       "        -5.7992e-02,  3.3845e-02,  1.3048e-02, -5.1361e-02,  7.8392e-02,\n",
       "        -1.9344e-02, -1.0448e-01,  4.1529e-02, -9.7657e-02, -3.4509e-03,\n",
       "         4.9083e-02,  5.5863e-02,  8.7877e-03, -1.1969e-01,  7.1582e-02,\n",
       "         2.4624e-02, -2.8234e-03, -1.0275e-01, -8.0798e-02, -1.2945e-01,\n",
       "         1.7228e-02, -8.7083e-02, -4.5541e-02, -3.6977e-02,  7.5634e-02,\n",
       "         6.3264e-02, -1.0102e-01, -9.6761e-02, -1.7960e-02, -1.6474e-02,\n",
       "         6.5089e-02, -5.6679e-02,  1.7903e-02, -6.3342e-02,  2.1894e-02,\n",
       "        -8.5694e-03, -2.0418e-02,  9.6943e-02,  6.6336e-02,  5.3024e-02,\n",
       "         7.7205e-02,  7.5687e-02, -2.4854e-02, -8.4196e-02,  7.2153e-02,\n",
       "        -3.3994e-02,  2.7743e-02,  7.6132e-02,  1.2271e-01,  8.2420e-02,\n",
       "         2.2781e-02,  6.0472e-03, -1.5400e-01, -1.1090e-01, -1.8680e-03,\n",
       "         9.7762e-02,  3.7373e-03, -2.6415e-02,  1.7530e-02,  9.8943e-03,\n",
       "        -4.3207e-02,  4.6805e-02,  1.3863e-02, -5.2318e-02, -3.4550e-03,\n",
       "        -3.7918e-02,  2.9433e-02,  3.3142e-02,  8.7807e-03,  3.0049e-02,\n",
       "         8.8094e-02,  1.4916e-03, -1.7431e-02, -2.5317e-02, -1.6277e-02,\n",
       "         1.1268e-02,  9.4293e-02,  3.3744e-02, -3.4135e-02,  6.1734e-04,\n",
       "        -5.8349e-02,  1.2800e-01,  2.4264e-03, -1.0573e-01, -2.0444e-02,\n",
       "         3.9112e-02, -1.4461e-01,  6.4038e-02, -8.3256e-03, -4.6320e-02,\n",
       "        -1.3400e-02,  1.2040e-02,  7.3522e-02, -1.6663e-02, -1.2628e-03,\n",
       "        -2.7094e-02, -1.8414e-03,  6.0205e-02, -6.7361e-02,  5.6380e-02,\n",
       "         2.3484e-03, -4.5203e-03,  4.1993e-02,  2.9977e-02, -1.2228e-02,\n",
       "         2.8904e-03, -1.7870e-02, -1.3307e-02, -4.5424e-02, -3.1245e-02,\n",
       "         4.0651e-03,  1.0091e-01,  6.3333e-02,  1.5903e-01,  9.9152e-02,\n",
       "        -2.0661e-02,  6.4784e-03,  1.3163e-03,  2.6181e-02, -9.9187e-03,\n",
       "         1.4386e-02, -4.5888e-02,  5.6548e-02,  3.5045e-02,  5.5262e-02,\n",
       "         3.0622e-02,  9.1758e-03, -1.0747e-01,  5.5859e-03, -5.0639e-02],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = hanlp.load(hanlp.pretrained.word2vec.MERGE_SGNS_BIGRAM_CHAR_300_ZH) # 加载word2vec词向量\n",
    "word2vec('中国')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 捕获了性别信息\n",
    "- 捕获了首都信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1429, device='cuda:0')\n",
      "tensor(0.0366, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.functional.cosine_similarity(\n",
    "    word2vec('国王')-word2vec('王妃'), \n",
    "    word2vec('男')-word2vec('女'), dim=0)\n",
    "      )\n",
    "print(torch.nn.functional.cosine_similarity(\n",
    "    word2vec('公主')-word2vec('王妃'), \n",
    "    word2vec('男')-word2vec('女'), dim=0)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4674, device='cuda:0')\n",
      "tensor(0.3933, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.functional.cosine_similarity(\n",
    "    word2vec('日本')-word2vec('东京'), \n",
    "    word2vec('中国')-word2vec('北京'), dim=0)\n",
    "      )\n",
    "print(torch.nn.functional.cosine_similarity(\n",
    "    word2vec('韩国')-word2vec('东京'), \n",
    "    word2vec('中国')-word2vec('北京'), dim=0)\n",
    "      )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算相似词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'上海': 0.6443496942520142, '天津': 0.6384099721908569, '西安': 0.611718475818634, '南京': 0.6113559603691101, '北京市': 0.6093109846115112, '海淀': 0.6049214601516724, '广州': 0.5977935791015625, '京城': 0.5955069661140442, '沈阳': 0.5865166187286377, '深圳': 0.580772876739502}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 单个词\n",
    "print(word2vec.most_similar('北京')) \n",
    "print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 获取基于上下文的词向量：语言模型的隐藏层表征\n",
    "\n",
    "同样以1.3中调用的[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "ckpt_path = \"uer/gpt2-chinese-cluecorpussmall\" # checkpoint模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(ckpt_path) # 分词器\n",
    "model = GPT2LMHeadModel.from_pretrained(ckpt_path) # 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========最后一层输出的内隐表征维度: ==========\n",
      "torch.Size([1, 25, 768])  1 x 输入字数 x 表征维度\n"
     ]
    }
   ],
   "source": [
    "model.config.output_hidden_states = True\n",
    "inputs = tokenizer('小明喜欢吃西瓜。小明喜欢打篮球。小明经常去花店', return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print('\\n' + '='*10 + '最后一层输出的内隐表征维度: ' + '='*10)\n",
    "print(str(outputs.hidden_states[-1].shape) + '  1 x 输入字数 x 表征维度')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 句法特征提取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 句法特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tre&nbsp;<br>───────&nbsp;<br>┌┬──┬──&nbsp;<br>││&nbsp;&nbsp;└─►&nbsp;<br>│└─►┌──&nbsp;<br>│&nbsp;&nbsp;&nbsp;└─►&nbsp;<br>└─────►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relat&nbsp;<br>─────&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">P&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>───────────────────────────────────────<br>_──────────────────────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_───────────────────►NP────┤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►VP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>_───►NP&nbsp;───┴►VP&nbsp;────►IP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>_──────────────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## workshop中的例子，研究中一般会把标点去掉，但是这里保留了标点，模型也是能够解析标点的\n",
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 选择使用的模型\n",
    "doc = Hanlp('欢迎大家参加工作坊！', tasks=['dep', 'con']) # 在tasks中选择需要的任务，如果不设置就进行所有任务（运行起来会慢一点）\n",
    "doc.pretty_print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 成分句法\n",
    "\n",
    "成分句法输出得到的是一个树结构的数据，可以看作一个嵌套的列表。我们可以：\n",
    "* 访问句法树的一些属性\n",
    "* 转换为括号表示法，计算括号数量\n",
    "* 访问句法树的子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "doc = Hanlp('欢迎大家参加工作坊！')\n",
    "tree = doc['con']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 2, 0, 0, 0)\n",
      "(0, 0, 2, 0, 1, 0, 0)\n",
      "(0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# 叶结点的位置\n",
    "for i in range(len(tree.leaves())):\n",
    "    print(tree.leaf_treeposition(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TOP(IP(VP(VV欢迎)(NP(PN大家))(IP(VP(VV参加)(NP(NN工作坊)))))(PU！)))'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转为括号表示法\n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '') # 去掉换行和空格\n",
    "bracket_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP(IP(VP(VV欢迎)(VP|<NP-IP>(NP(PN大家))(IP(VP(VV参加)(NP(NN工作坊))))))(PU！)))\n"
     ]
    }
   ],
   "source": [
    "# 转换为Chomsky Normal Form，可以用tree.un_chomsky_normal_form()转换回来\n",
    "tree.chomsky_normal_form() \n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '')\n",
    "print(bracket_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TOP|IP(VP(VV欢迎)(VP|<NP-IP>(NP(PN大家))(IP|VP(VV参加)(NP(NN工作坊)))))(PU！))'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出中有些节点只派生出一支，是冗余的（例如最外层的TOP根结点只派生出IP，以及句子中的IP只派生出VP），可以选择压缩节点\n",
    "tree.collapse_unary(collapseRoot=True, joinChar='|') # 压缩冗余节点，压缩的节点用｜来表示\n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '')\n",
    "bracket_form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((()((())(()(()))))())\n",
      "左括号数: [3, 3, 2, 2, 1]\n",
      "右括号数: [1, 2, 1, 5, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>left_bracket</th>\n",
       "      <th>right_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>欢迎</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>大家</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>参加</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>工作坊</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>！</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word left_bracket right_bracket\n",
       "0   欢迎            3             1\n",
       "1   大家            3             2\n",
       "2   参加            2             1\n",
       "3  工作坊            2             5\n",
       "4    ！            1             2"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "# 计算括号表示法中每个词的括号数\n",
    "bracket_clean= re.sub(\"([^()])\", \"\", bracket_form) # 只保留括号\n",
    "print(bracket_clean)\n",
    "\n",
    "# 计算左括号数\n",
    "left_bracket = [len(re.findall(\"\\(\", i)) for i in bracket_clean] \n",
    "left_bracket_count = []\n",
    "for i in left_bracket:\n",
    "    if len(left_bracket_count) == 0 or (i == 1 and j != 1):\n",
    "        left_bracket_count.append(1)\n",
    "    elif i == 1 and j == 1:\n",
    "        left_bracket_count[-1] += 1\n",
    "    j = i\n",
    "print(\"左括号数:\", left_bracket_count)\n",
    "\n",
    "# 计算右括号数\n",
    "right_bracket = [len(re.findall(\"\\)\", i)) for i in bracket_clean] \n",
    "right_bracket_count = []; j = 0\n",
    "for i in right_bracket:\n",
    "    if i == 1 and j != 1:\n",
    "        right_bracket_count.append(1)\n",
    "    elif i == 1 and j == 1:\n",
    "        right_bracket_count[-1] += 1\n",
    "    j = i\n",
    "print(\"右括号数:\", right_bracket_count)\n",
    "\n",
    "# 可以保存为 dataframe 进行进一步的句法特征分析\n",
    "df_bracket = pd.DataFrame([tree.leaves(), left_bracket_count, right_bracket_count]).T\n",
    "df_bracket.columns = ['word', 'left_bracket', 'right_bracket']\n",
    "# df_bracket.to_csv('bracket.csv', index=False) # 保存为csv文件\n",
    "df_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal nodes: ['欢迎', '大家', '参加', '工作坊', '！']\n",
      "Tree depth: 7\n",
      "Tree productions: [TOP|IP -> VP PU, VP -> VV VP|<NP-IP>, VV -> '欢迎', VP|<NP-IP> -> NP IP|VP, NP -> PN, PN -> '大家', IP|VP -> VV NP, VV -> '参加', NP -> NN, NN -> '工作坊', PU -> '！']\n",
      "Part of Speech: [('欢迎', 'VV'), ('大家', 'PN'), ('参加', 'VV'), ('工作坊', 'NN'), ('！', 'PU')]\n"
     ]
    }
   ],
   "source": [
    "# 句法树的属性\n",
    "print(\"Terminal nodes:\", tree.leaves())\n",
    "print(\"Tree depth:\", tree.height())\n",
    "print(\"Tree productions:\", tree.productions())\n",
    "print(\"Part of Speech:\", tree.pos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP|IP\n",
      "  (VP\n",
      "    (VV 欢迎)\n",
      "    (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "  (PU ！))\n",
      "(VP (VV 欢迎) (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "(VV 欢迎)\n",
      "(VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊))))\n",
      "(NP (PN 大家))\n",
      "(PN 大家)\n",
      "(IP|VP (VV 参加) (NP (NN 工作坊)))\n",
      "(VV 参加)\n",
      "(NP (NN 工作坊))\n",
      "(NN 工作坊)\n",
      "(PU ！)\n"
     ]
    }
   ],
   "source": [
    "# 句法树的嵌套结构\n",
    "for i in tree.subtrees():  # 根据Tree productions，遍历所有的子树，每一棵子树都是一个Tree对象，可以进行之前相同的操作\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (0,),\n",
       " (0, 0),\n",
       " (0, 0, 0),\n",
       " (0, 1),\n",
       " (0, 1, 0),\n",
       " (0, 1, 0, 0),\n",
       " (0, 1, 0, 0, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 1, 0),\n",
       " (0, 1, 1, 0, 0),\n",
       " (0, 1, 1, 1),\n",
       " (0, 1, 1, 1, 0),\n",
       " (0, 1, 1, 1, 0, 0),\n",
       " (1,),\n",
       " (1, 0)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过索引访问句法树的子树\n",
    "treepositions = tree.treepositions() # 所有节点的索引\n",
    "treepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP|IP\n",
      "  (VP\n",
      "    (VV 欢迎)\n",
      "    (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "  (PU ！))\n",
      "(VP (VV 欢迎) (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "(VV 欢迎)\n",
      "欢迎\n",
      "(VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊))))\n",
      "(NP (PN 大家))\n",
      "(PN 大家)\n",
      "大家\n",
      "(IP|VP (VV 参加) (NP (NN 工作坊)))\n",
      "(VV 参加)\n",
      "参加\n",
      "(NP (NN 工作坊))\n",
      "(NN 工作坊)\n",
      "工作坊\n",
      "(PU ！)\n",
      "！\n"
     ]
    }
   ],
   "source": [
    "for i in treepositions: # 遍历所有节点\n",
    "    print(tree[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 依存句法\n",
    "* 依存句法的数据结构更加简单，为一个列表`[(head, relation), ... ]`。列表中第$i$个值中包括了它的核心词的位置以及它与核心词之间的依存关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'root'), (1, 'dobj'), (1, 'dep'), (3, 'dobj'), (1, 'punct')]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "doc = Hanlp('欢迎大家参加工作坊！')\n",
    "doc['dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>head</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>欢迎</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>大家</td>\n",
       "      <td>1</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>参加</td>\n",
       "      <td>1</td>\n",
       "      <td>dep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>工作坊</td>\n",
       "      <td>3</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>！</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  head    rel\n",
       "0   欢迎     0   root\n",
       "1   大家     1   dobj\n",
       "2   参加     1    dep\n",
       "3  工作坊     3   dobj\n",
       "4    ！     1  punct"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以保存为 dataframe 进行进一步的句法特征分析\n",
    "df_dep = pd.DataFrame(doc['dep'], columns=['head', 'rel'])\n",
    "df_dep['word'] = doc['tok/fine']\n",
    "df_dep = df_dep[['word', 'head', 'rel']]\n",
    "df_dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 批量操作\n",
    "\n",
    "只需要将要处理的句子放在list中，一起进行特征抽取即可。这对所有特征都适用，不仅是句法特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tree&nbsp;<br>────────&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;┌─►&nbsp;<br>┌───►└──&nbsp;<br>│&nbsp;&nbsp;&nbsp;┌──►&nbsp;<br>│&nbsp;&nbsp;&nbsp;│┌─►&nbsp;<br>│┌─►└┴──&nbsp;<br>││┌─►┌──&nbsp;<br>│││&nbsp;&nbsp;└─►&nbsp;<br>└┴┴──┬──&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relati&nbsp;<br>──────&nbsp;<br>nummod&nbsp;<br>nsubj&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nsubj&nbsp;&nbsp;<br>prep&nbsp;&nbsp;&nbsp;<br>pobj&nbsp;&nbsp;&nbsp;<br>root&nbsp;&nbsp;&nbsp;<br>punct&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;<br>──&nbsp;<br>NT&nbsp;<br>M&nbsp;&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>P&nbsp;&nbsp;<br>NR&nbsp;<br>VV&nbsp;<br>PU&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">NER&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>────────────&nbsp;<br>───►DATE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>───►LOCATION&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>────────────&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARGM-TMP&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;├►ARG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARGM-LOC&nbsp;<br>╟──►PRED&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;<br>────────────────────────────────<br>NT──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>M&nbsp;───►CLP&nbsp;──┴►QP&nbsp;───┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>NN──┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►NP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>NN&nbsp;&nbsp;├────────►NP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>NN──┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>P&nbsp;──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>NR───►NP&nbsp;───┴►PP&nbsp;───┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>VV───────────►VP&nbsp;───┴►VP────┤&nbsp;&nbsp;&nbsp;<br>PU──────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div><br><div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tre&nbsp;<br>───────&nbsp;<br>┌┬──┬──&nbsp;<br>││&nbsp;&nbsp;└─►&nbsp;<br>│└─►┌──&nbsp;<br>│&nbsp;&nbsp;&nbsp;└─►&nbsp;<br>└─────►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relat&nbsp;<br>─────&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;<br>──&nbsp;<br>VV&nbsp;<br>PN&nbsp;<br>VV&nbsp;<br>NN&nbsp;<br>PU&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA1&nbsp;&nbsp;<br>────────&nbsp;<br>╟──►PRED&nbsp;<br>───►ARG1&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARG2&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA2&nbsp;&nbsp;<br>────────&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>╟──►PRED&nbsp;<br>───►ARG1&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>────────────────────────────────────────<br>VV──────────────────────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>PN───────────────────►NP────┤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>VV──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►VP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>NN───►NP&nbsp;───┴►VP&nbsp;────►IP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>PU──────────────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = ['2023年心理语言学会在广州召开。', '欢迎大家参加工作坊！']\n",
    "docs = Hanlp(sentences)\n",
    "docs.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量为: 2\n",
      "['2023', '年', '心理', '语言', '学会', '在', '广州', '召开', '。']\n",
      "['欢迎', '大家', '参加', '工作坊', '！']\n"
     ]
    }
   ],
   "source": [
    "# 提取出来的特征直接索引即可\n",
    "print(\"句子数量为:\", docs.count_sentences())\n",
    "for i in range(docs.count_sentences()):\n",
    "    print(docs['tok/fine'][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 语言任务\n",
    "在本小节中，我们以主题分析任务和上下文学习为例，演示语言模型的加载和推理过程。对于其他语言任务，均可在huggingface平台搜索到类似的教程文档以及代码。\n",
    "## 3.1 主题分析任务\n",
    "使用transformers管道pipeline快速实现语言任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 单个句子主题分析计算 ====================\n",
      "\n",
      "Input: 欢迎参加工作坊！\n",
      "Prediction: culture, Score: 0.723\n",
      "\n",
      "\n",
      "\n",
      "==================== 多个句子批量进行主题分析计算 ====================\n",
      "\n",
      "Input: 2023年心理语言学会在广州召开\n",
      "Prediction: culture, Score: 0.969\n",
      "\n",
      "Input: 湖人有意签保罗补强，联手詹姆斯追逐总冠军\n",
      "Prediction: sports, Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = 'model/roberta-base-finetuned-chinanews-chinese'\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 利用pipeline快速进行语言任务\n",
    "text = '欢迎参加工作坊！'\n",
    "text_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "res = text_classification(text)[0]\n",
    "print(\"=\"*20, \"单个句子主题分析计算\", \"=\"*20)\n",
    "print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")\n",
    "\n",
    "\n",
    "# pipeline可以实现批量句子的计算\n",
    "text_lst = ['2023年心理语言学会在广州召开', '湖人有意签保罗补强，联手詹姆斯追逐总冠军']\n",
    "res_lst = text_classification(text_lst)\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"多个句子批量进行主题分析计算\", \"=\"*20)\n",
    "for text, res in zip(text_lst, res_lst):\n",
    "    print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 上下文学习\n",
    "通过在上下文中给定任务描述和示例，通用的文本生成模型可以根据上下文快速学习语言任务。在这里我们不使用pipeline，直接调用模型方法进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现文本翻译 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang/anaconda3/envs/ngram/lib/python3.7/site-packages/transformers/generation/utils.py:1278: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: translate English to German: How old are you?\n",
      "Output: Wie alte sind Sie?\n",
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现主题文本生成 ====================\n",
      "Input: Generate sentences with the topic : \n",
      "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
      "entertainment => \n",
      "\n",
      "Output: a new tv series starring adrian sandler is \n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现文本翻译\", \"=\"*20)\n",
    "text = \"translate English to German: How old are you?\"\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现主题文本生成\", \"=\"*20)\n",
    "text = '''Generate sentences with the topic : \n",
    "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
    "entertainment => \n",
    "'''\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 文本生成超参数\n",
    "在本小节中，我们会分析文本生成中的温度参数、搜索策略参数以及top-p参数对文本生成结果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Welcome to \n",
      "\n",
      "==================== 贪婪搜索 ====================\n",
      "Iter 0: Welcome to the e-commerce world!\n",
      "Iter 1: Welcome to the e-commerce world!\n",
      "Iter 2: Welcome to the e-commerce world!\n",
      "Iter 3: Welcome to the e-commerce world!\n",
      "Iter 4: Welcome to the e-commerce world!\n",
      "==================== 随机搜索, 温度参数=0.1 ====================\n",
      "Iter 0: Welcome to the iStockphoto\n",
      "Iter 1: Welcome to the official website of the \n",
      "Iter 2: Welcome to the world of e-commerce\n",
      "Iter 3: Welcome to the world of e-commerce\n",
      "Iter 4: Welcome to the official website of the \n",
      "==================== 随机搜索, 温度参数=1.0 ====================\n",
      "Iter 0: Welcome to World of Aliens! a\n",
      "Iter 1: Welcome to the new website.  All\n",
      "Iter 2: Welcome to the Wikimedia Foundation! This\n",
      "Iter 3: Hi, I am Jason, the owners of\n",
      "Iter 4: Welcome to the Official Website of eW\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "text = 'Welcome to '\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 其余可修改参数包括top_k, top_p等, 可直接在.generate()方法中调用\n",
    "# ref: https://huggingface.co/blog/how-to-generate\n",
    "print(f'\\nInput: {text}\\n')\n",
    "print(\"=\"*20, \"贪婪搜索\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "print(\"=\"*20, \"随机搜索, 温度参数=0.1\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=0.1, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "\n",
    "print(\"=\"*20, \"随机搜索, 温度参数=1.0\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=1.0, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd65688f949d2ad1e75c78c2f6ddcc07f0a0c35d5c97b06568571046f968236a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
